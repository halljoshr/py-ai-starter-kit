# Feature Discussion: py-ai-starter-kit Overall Project

**Date:** 2026-01-28
**Phase:** /discuss overall-project
**Status:** In Progress

---

## Decisions Made

### 1. **Primary Goal**
**Decision:** Open-source toolkit, validated with company use first

**Rationale:** Build for broader Python community but ensure battle-tested through real company usage before public release. This ensures quality and real-world validation.

---

### 2. **GSD Relationship**
**Decision:** Independent development, borrow philosophies

**Rationale:** Don't depend on external developer. Take GSD's winning concepts (fresh context per task, STATE.md, pause/resume) but implement in PIV style with our own opinions and flexibility.

---

### 3. **Workflow Philosophy**
**Decision:** PIV is the target workflow, current manual process is temporary

**Rationale:** The documented PIV flow (Prime → Discuss → Spec → Plan → Execute → Validate → Commit) is what we're building toward. Current process exists because skills aren't production-ready yet. Dogfooding will make PIV actually work.

---

### 4. **Critical Blocker**
**Decision:** /prime token efficiency is the #1 pain point

**Rationale:** On large codebases, /prime uses too many tokens (40-50K). Blocks ability to use full PIV workflow. Must be solved first.

---

### 5. **Prime Strategy**
**Decision:** Two modes - /prime (normal) and /prime-deep

**Rationale:** Normal mode ~8-10K tokens for known codebases. Deep mode ~40-50K for unfamiliar/complex codebases. Already designed, just needs implementation.

---

### 6. **Goal Scope**
**Decision:** Keep all 21 goals in GOALS.md

**Rationale:** Need to validate on massive projects (the only ones user manages). Can't simplify or reduce scope - must prove it scales from day one.

---

### 7. **State Management**
**Decision:** YAML files (machine-readable) + /status skill generates markdown reports (human-readable)

**Rationale:**
- **Source of truth:** session.yaml, task-NNN.yaml, agents.yaml (structured, queryable)
- **Human interface:** /status generates STATE.md on-demand (readable snapshot)
- Separates concerns: machines work with YAML, humans read markdown

---

### 8. **TDD Enforcement**
**Decision:** Strict TDD by default, opt-out requires written justification in task YAML

**Rationale:** Quality is non-negotiable. To prevent "easy way out," make opting-out harder than following TDD. Task must include `tdd_opt_out: true` + substantive rationale. May escalate to user approval if needed.

---

### 9. **Swarm Architecture** ⭐ CRITICAL DECISION
**Decision:** Swarm-lite approach - Build state structure now, add automation later

**Details:**
- **Today (Priority 1):** Build swarm YAML state structure
  - agents.yaml (agent registry)
  - task-NNN.yaml (task ownership)
  - messages.yaml (coordination)
  - session.yaml (session state)
  - STATE.md generated by /status skill
  - Manual multi-session: Run multiple Claude windows, coordinate via shared YAML

- **Future (Priority 2-3):** Add automation
  - Orchestrator spawns agents via Task tool with run_in_background
  - Automatic agent lifecycle management
  - Parallel task execution
  - Inter-agent communication protocols

**Rationale:**
1. **Solves immediate problem:** Fresh 50K context per executor (prevents context rot on large projects)
2. **No refactoring tax:** Build with swarm state from day one, just add automation later
3. **Validates at scale:** Swarm state structure handles massive projects (agent task ownership prevents conflicts)
4. **Keeps dogfooding tractable:** Avoid automation complexity during Priority 1. Manual multi-session proves design first.
5. **GSD-inspired:** Borrows proven pattern (fresh context per task, STATE.md coordination)

**Future Enhancement Note:**
Add to Priority 2 or 3 goals:
- **Goal: Automatic Agent Orchestration**
  - Orchestrator skill spawns researcher/executor/reviewer agents
  - Background task execution via Task tool
  - Agent health monitoring and error recovery
  - Parallel execution with token budget management per agent
  - Prerequisites: Priority 1 complete, swarm state stable

---

### 10. **Prime Normal Mode Scope**
**Decision:** Read structure + core docs + git state + samples, skip heavy docs

**Reads (~8-10K tokens):**
- Project structure (git ls-files, directory tree)
- CLAUDE.md and README.md (core instructions)
- Recent git state (status, last 10 commits, current branch)
- Sample 1-2 files (entry point, service, test - see patterns in action)

**Skips (load on-demand):**
- Reference docs (.claude/reference/*.md) - 15 docs, ~200K total
- Skill files (.claude/skills/*/SKILL.md) - Already available via Claude Code
- Research docs (.agents/research/*.md) - Historical, read when needed
- Full file contents - Use search patterns (--files-with-matches), read during implementation

**Rationale:** Shallow discovery provides enough context to start. Deep reading happens during implementation when needed. Massive token savings on large codebases.

---

### 11. **Prime Deep Mode Usage**
**Decision:** Use /prime-deep for orchestrators, spec creation, and complex scenarios

**Use /prime-deep when:**
- First time on codebase (comprehensive understanding needed)
- Major refactoring (cross-cutting concerns)
- Complex multi-subsystem features
- Onboarding new developers
- **Orchestrator role** (needs full context to coordinate agents)
- **Building specs** (/discuss → /spec phase needs big picture)

**Use /prime normal when:**
- Executor role (focused on single task with 50K fresh context)
- Known codebase, small changes
- Bug fixes
- Simple single-file features

**Rationale:** Orchestrators and spec-writers need comprehensive understanding to make good decisions. Executors work on focused tasks with sufficient context from task-NNN.yaml.

---

### 12. **Token Budget Thresholds**
**Decision:** Warning at 75% (150K), Critical at 88% (175K)

**Thresholds:**
- 25% (50K): Log progress
- 50% (100K): Halfway checkpoint
- 75% (150K): **WARNING** - Consider checkpointing soon
- 88% (175K): **CRITICAL** - Automatic checkpoint procedure

**Rationale:** Research-backed thresholds from GSD and context-optimization studies. Proven to prevent context rot while maximizing usage. Gives enough margin for graceful checkpoint.

---

### 13. **Caching Strategy**
**Decision:** Basic caching + full instrumentation in Priority 1

**Priority 1 Scope:**
- Cache CLAUDE.md and reference docs (static, high-reuse)
- Use Anthropic's prompt caching API
- Full instrumentation: tokens used, cache hits/misses, cost calculations
- Measure actual impact on massive projects during dogfooding

**Future (Priority 3-4):**
- Full caching strategy (system prompts 1hr TTL, conversation 5min TTL)
- Speculative caching (warm while user types)
- Advanced optimization per research (60-90% cost savings)

**Rationale:** Data-driven approach. Simple caching with comprehensive measurement lets us see real impact and decide on advanced optimizations later. Low risk, immediate learning.

---

## Architecture: Task #1 - System Architecture (COMPLETE ✓)

### Swarm State Structure (Decided)

```
.agents/state/
├── STATE.md              # Human-readable snapshot (generated by /status)
├── session.yaml          # Session metadata (feature, phase, token budget)
├── agents.yaml           # Agent registry (roles, status, capabilities)
├── messages.yaml         # Agent communication log
└── task-NNN.yaml         # Individual task files (status, owner, results)
```

### Agent Architecture (Designed, Not Implemented)

**Current (Priority 1):**
- Mode: `single`
- Agent: `main` (orchestrator role, does everything)
- Manual multi-session coordination via shared YAML state

**Future (Priority 2-3):**
- Mode: `swarm`
- Agents:
  - 1x orchestrator (coordinates, assigns tasks)
  - 4x researchers (parallel research: stack, features, architecture, pitfalls)
  - 3x executors (parallel task execution, 50K budget each)
  - 1x reviewer (code review)
  - 1x debugger (error analysis and fixes)

### Skill Invocation Pattern

**Clarified:** All agents have access to all skills (Claude Code feature)

**Coordination:**
- Skills read/write shared YAML state
- Task ownership via `owner` field in task-NNN.yaml
- Orchestrator assigns tasks, executors claim and complete them

---

## Token Management: Task #2 - Token Management and Prime Optimization (COMPLETE ✓)

**Decisions made in section 10-13 above**

---

### 14. **PIV Workflow Session Breaks**
**Decision:** Breaks between phases (multi-session by design)

**Natural Checkpoint Points:**
1. After /prime (context established)
2. After /discuss (decisions made)
3. After /spec (specification complete)
4. After /plan (tasks ready for execution)
5. After each /execute phase (Phase 1 done, Phase 2 starts new session)
6. After /validate (all quality gates passed)
7. After /commit (feature complete)

**Typical Feature:** 5-7 sessions, each with fresh context

**Rationale:** Prevents context rot, aligns with swarm-lite (50K per executor), allows parallel execution, provides natural resume points.

---

### 15. **Session Resume Strategy**
**Decision:** Read session.yaml + artifacts + conditional re-prime

**On /resume:**
1. Read `session.yaml` (feature name, current phase, status, next steps)
2. Read relevant artifacts from previous phase:
   - After /discuss → read `.agents/research/{feature}-discussion-{date}.md`
   - After /plan → read `.agents/plans/{feature}.md` and `task-NNN.yaml` files
   - After /execute → read `STATE.md` and completed task results
3. Check git state: `git log -1`, `git status`
4. **Conditional re-prime:** If commits since last session OR uncommitted changes → run `/prime --quick`
5. Display STATE.md summary

**Rationale:** Efficient context restoration. Only re-prime if codebase actually changed. Saves tokens while staying current.

---

### 16. **Execute Scope**
**Decision:** One phase at a time

**Behavior:**
- `/execute` runs all tasks in current phase (e.g., Phase 1)
- Stops at phase boundary
- Updates session.yaml: `phase: execute-phase-1-complete`
- Next session: `/resume` → `/execute` (runs Phase 2)

**Why not run all phases?**
- Each executor gets fresh 50K context per phase
- Natural checkpoint points
- Safe for massive features (10+ tasks)
- Aligns with swarm-lite architecture

**Rationale:** Predictable token usage, fresh context per phase, safe for large features.

---

### 17. **Validation Timing**
**Decision:** After each task (quality-first)

**Behavior:**
- Each task-NNN.yaml includes `verify` command
- After task completion, /execute runs the verify command
- If verification fails, task marked `failed`, stop execution
- User must fix issue before continuing

**Example:**
```yaml
# task-001.yaml
verify: "uv run pytest tests/unit/test_user_model.py -v"
```

After implementing task-001:
```bash
uv run pytest tests/unit/test_user_model.py -v
# ✅ Pass → mark task complete, continue
# ❌ Fail → mark task failed, stop, report issue
```

**Rationale:** Catch issues early, prevent cascading failures, maintain quality throughout. Small cost (run tests 10+ times) worth it for quality.

---

### 18. **Commit Strategy**
**Decision:** One atomic commit per task (GSD-style)

**Behavior:**
- After task completes AND verification passes → automatic commit
- Commit message format:
  ```
  <type>(scope): <task-name>

  Task: task-NNN
  Verify: <verify-command> passed

  Co-Authored-By: Claude <noreply@anthropic.com>
  ```
- Git history: 10-20+ commits per feature
- Easy to bisect, granular revert

**Example:**
```bash
# task-001 complete
git commit -m "feat(models): Add User model with email validation

Task: task-001
Verify: pytest tests/unit/test_user_model.py passed

Co-Authored-By: Claude <noreply@anthropic.com>"
```

**Rationale:** Git bisect friendly, fine-grained revert capability, atomic changes, excellent for massive projects with complex history.

---

## Core PIV Workflows: Task #3 - Core Workflows (COMPLETE ✓)

**Decisions made in section 14-18 above**

---

### 19. **TDD Workflow Implementation**
**Decision:** Test-first, single atomic commit per task

**Workflow:**
1. **Read task requirements** from task-NNN.yaml
2. **Write failing test first**
   - Test must fail (proves it's testing something)
   - Covers the requirements from task description
3. **Implement code to pass test**
   - Minimal implementation to make test pass
   - Follow patterns from task description
4. **Refactor if needed**
   - Improve code quality without changing behavior
   - Test must still pass
5. **Single atomic commit**
   ```
   feat(scope): Implement X with tests

   Task: task-NNN
   Verify: <verify-command> passed
   Coverage: 85% on modified files

   Co-Authored-By: Claude <noreply@anthropic.com>
   ```

**Enforcement:**
- /execute checks: Is test written before implementation?
- If TDD skipped without opt-out → FAIL task, require fix
- If opt-out present → validate justification (see Decision #23)

**Rationale:** Ensures tests are real (fail first), maintains quality, keeps commits atomic, pragmatic approach.

---

### 20. **Per-Task Validation Checks**
**Decision:** Comprehensive validation after each task

**Validation Steps (in order):**
1. **Task-specific verify command** (from task-NNN.yaml)
   - Example: `uv run pytest tests/unit/test_user_model.py -v`
   - Fast, focused on new code
2. **Linting** (`uv run ruff check .`)
   - Code style compliance
   - ~2 seconds
3. **Type checking** (`uv run mypy app/`)
   - Type hint validation
   - ~5-10 seconds
4. **All unit tests** (`uv run pytest tests/unit/ -v`)
   - Regression prevention
   - ~10-30 seconds

**If any check fails:**
- Mark task as `failed` in task-NNN.yaml
- Stop execution (don't proceed to next task)
- Report failure details to user
- Require fix before continuing

**Rationale:** Catch all issues immediately. Fast feedback loop. Prevent bad code from getting committed.

---

### 21. **Integration/E2E Test Timing**
**Decision:** After each phase (phase boundaries)

**Test Tiers:**
- **Per-task:** Unit tests only (fast, ~30 seconds)
- **Per-phase:** Integration tests (medium, ~1-3 minutes)
- **End of feature:** E2E tests (slow, ~5-10 minutes)

**Example Feature:**
```
Phase 1 (Data Models):
  - task-001: unit tests ✓
  - task-002: unit tests ✓
  - task-003: unit tests ✓
  → Phase 1 complete → Run integration tests

Phase 2 (Service Layer):
  - task-004: unit tests ✓
  - task-005: unit tests ✓
  → Phase 2 complete → Run integration tests

Phase 3 (API Endpoints):
  - task-006: unit tests ✓
  - task-007: unit tests ✓
  → Phase 3 complete → Run integration tests

Feature complete → Run E2E tests
```

**Rationale:** Balance speed and safety. Fast per-task feedback, catch integration issues at phase boundaries before they cascade.

---

### 22. **Coverage Enforcement**
**Decision:** Hybrid - New code 80%+, overall >= 80%

**Two-Level Enforcement:**

**Level 1: New Code Coverage (Strict)**
```bash
# Per-task: Measure coverage only on files modified in this task
uv run pytest --cov=app --cov=src \
  --cov-report=term-missing \
  --cov-report=json

# Parse JSON, filter to modified files
# Require: Each modified file >= 80% coverage
```

**Level 2: Overall Project Coverage (Maintain)**
```bash
# Per-phase: Measure entire codebase
uv run pytest --cov=app --cov=src --cov-fail-under=80

# Must pass: Overall coverage >= 80%
```

**Failure Handling:**
- New code < 80% → Task fails, require more tests
- Overall < 80% → Phase fails, must improve before proceeding
- Display: Which lines not covered (--cov-report=term-missing)

**Rationale:** Strict on new code (must be well-tested), maintain overall quality (can't degrade), clear feedback on what needs coverage.

---

### 23. **TDD Opt-Out Validation**
**Decision:** User approval required

**Workflow:**
```yaml
# task-005.yaml
tdd_opt_out: true
tdd_rationale: |
  This task refactors internal helper functions. Tests already exist
  for the public API that uses these helpers. No new behavior being
  added, just improving code organization.
```

**When /execute encounters opt-out:**
1. Stop execution
2. Use AskUserQuestion:
   ```
   Task task-005 requests TDD opt-out.

   Rationale:
   "This task refactors internal helper functions. Tests already exist
   for the public API that uses these helpers..."

   Approve TDD opt-out?
   - Yes, rationale is valid
   - No, require TDD for this task
   ```
3. User decision:
   - **Yes** → Proceed without test-first requirement
   - **No** → Mark task blocked, require TDD implementation

**Still Required (even with opt-out):**
- Task must have some test coverage
- Coverage thresholds still apply (80%)
- Validation still runs

**Rationale:** Human oversight prevents shortcuts. User makes final call on quality tradeoffs. Accountability for decisions.

---

## Quality & Testing: Task #4 - Quality Gates and TDD Enforcement (COMPLETE ✓)

**Decisions made in section 19-23 above**

---

### 24. **Priority 1 Implementation Order**
**Decision:** Order by immediate value (pragmatic approach)

**Priority 1 Goals (5 total):**

**#1: Prime Optimization (FIRST - Critical Blocker)**
- Implement /prime normal mode (~8-10K tokens)
- Implement /prime-deep mode (~40-50K tokens)
- Shallow discovery: skip reference/skill/research docs
- Use when: Normal for executors, Deep for orchestrators/specs
- **Unblocks:** Everything else (can't dogfood without efficient prime)
- **Timeline:** Week 1-2

**#2: Spec Creation Process (SECOND - Needed for Dogfooding)**
- Build /discuss skill (interactive design decisions)
- Build /spec skill (generate Anthropic XML specification)
- Create spec templates (API, data processing, refactoring)
- Integrate with /plan (consume specs)
- **Enables:** Systematic feature development, this very dogfooding process
- **Timeline:** Week 2-3

**#3: Session Management (THIRD - Foundation)**
- Implement session.yaml, task-NNN.yaml, agents.yaml, messages.yaml
- Build /pause skill (checkpoint state)
- Build /resume skill (restore with conditional re-prime)
- Build /status skill (generate STATE.md from YAML)
- **Enables:** Multi-session workflow, state persistence
- **Timeline:** Week 3-4

**#4: Multi-Session Architecture (FOURTH - Uses Session Mgmt)**
- Implement phase-based execution (/execute runs one phase)
- Session break points (after each phase)
- Fresh context per phase (50K per executor)
- **Depends on:** Session management (#3)
- **Timeline:** Week 4-5

**#5: Comprehensive Skills Tree (FIFTH - Polish)**
- Complete all 24 atomic skills
- Build compound skills (quick-fix, refactor-safe, investigate)
- Document skill coordination patterns
- **Uses:** Session management, multi-session architecture
- **Timeline:** Week 5-6

**Token Budget Management (Embedded throughout):**
- Basic caching (CLAUDE.md, reference docs) - Goal #4, Week 1
- Token warnings (75%, 88%) - Goal #3, Week 3
- Full instrumentation (metrics, costs) - Goal #4, Week 4

**Rationale:** Unblock critical path first (Prime), enable dogfooding quickly (Spec), build infrastructure (Session/Multi-session), polish (Skills).

---

## Priority 1 Implementation: Task #5 (COMPLETE ✓)

**Decisions made in section 24 above**

---

## Priority 2-4 Roadmap: Task #6 (DEFERRED)

**Decision:** Keep all 21 goals in GOALS.md as documented

**Approach:**
- Priority 1: 5 goals (6-week timeline above)
- Priority 2-4: 16 goals remain as documented in GOALS.md
- No changes needed - goals are well-defined
- Priorities validated during dogfooding
- Revisit after Priority 1 complete

**Rationale:** Don't over-plan. Priority 1 will teach us what's actually needed. Goals document is comprehensive and well-thought-out. Trust the research.

---

### 25. **Goal #6: JSON Features Tracking (Deferred)**
**Decision:** Defer to Priority 4 via YAGNI

**Rationale:** Session tracking (session.yaml, task-NNN.yaml, STATE.md) handles current-feature needs. Historical cross-feature tracking is nice-to-have but not critical. Build session tracking first, add historical analytics later if dogfooding reveals need. Potentially merge with Goal #18 (Feedback Loop Analytics).

---

### 26. **Goal #7: TDD Culture Beyond Enforcement**
**Decision:** Training docs + plan enforcement + metrics tracking

**Implementation:**
1. **Training Documentation** - `.claude/reference/tdd-best-practices.md`
   - Why TDD matters
   - How to write good tests
   - Common patterns and anti-patterns
   - Link from CLAUDE.md

2. **Plan Enforcement** - `/plan` skill generates test requirements
   - Each task includes: test cases, acceptance criteria, verify command
   - Forces thinking about tests during planning phase
   - Structural enforcement (can't skip planning tests)

3. **Metrics Tracking** - `.agents/analytics/tdd-metrics.json`
   - Track: % tasks with TDD, % opt-outs, test quality scores
   - Dashboard shows adoption over time
   - Data-driven culture building

**Rationale:** Multi-pronged approach - education + structure + measurement. Makes TDD the path of least resistance.

---

### 27. **Goal #9: Profiling & Optimization System**
**Decision:** Compound skill with co-located Python helper scripts

**Structure Pattern (from coleam00/second-brain-skills):**
```
.claude/skills/optimize/
├── SKILL.md                         # Orchestration instructions
├── scripts/                         # Helper scripts subdirectory
│   ├── profile_cpu.py               # cProfile wrapper
│   ├── profile_memory.py            # memory_profiler wrapper
│   ├── profile_io.py                # I/O bottleneck detection
│   ├── benchmark.py                 # pytest-benchmark wrapper
│   └── generate_report.py           # Consolidate results to markdown
├── templates/                       # Report templates
│   └── performance_report.md.j2     # Jinja2 template
└── examples/                        # Example outputs
    └── sample-report.md
```

**Compound Skill: /optimize-performance**
- Runs all profilers sequentially
- Generates consolidated report
- Provides optimization recommendations
- Saves to `.agents/reports/performance-{date}.md`

**SKILL.md calls helpers:**
```bash
uv run python .claude/skills/optimize/scripts/profile_cpu.py --target src/app/main.py
uv run python .claude/skills/optimize/scripts/profile_memory.py --target src/app/main.py
uv run python .claude/skills/optimize/scripts/benchmark.py --suite tests/benchmarks/
uv run python .claude/skills/optimize/scripts/generate_report.py --output .agents/reports/
```

**Benefits:**
- Consistent results (same script, same output)
- Testable (scripts have their own tests)
- Reusable (can run manually or from other skills)
- Maintainable (Python code easier than bash in markdown)

**Rationale:** Proven pattern from real-world Claude Code skills. Black box approach (one skill, comprehensive output). Defers granular atomic skills (/profile-cpu, /profile-memory) to Priority 3 if needed.

---

## Priority 2: Task #8 Complete ✓

**Decisions for Goals #6, #7, #9**

---

### 28. **Goal #10: Language-Specific Standards**
**Decision:** YAGNI - Add when actively using

**Approach:**
- Python complete (15 reference docs in `.claude/reference/`)
- Add TypeScript/Go/etc only when starting projects in those languages
- Don't pre-build infrastructure you don't need
- Template structure exists (can copy Python pattern)

**Rationale:** Focus on perfecting Python patterns first. Expanding to other languages is straightforward once patterns are proven.

---

### 29. **Goal #11: Project Ownership & Documentation**
**Decision:** Defer formalization - Current approach working

**Current State:**
- `.agents/decisions/` - Decision tracking ✓
- `.agents/research/` - Research and analysis ✓
- Discussion files - Architectural decisions ✓

**Defer to post-dogfooding:**
- Formal ADR structure (`.agents/architecture/adr/`)
- OWNERS files (`.github/OWNERS.yaml`)
- Onboarding documentation

**Rationale:** Ad-hoc approach is working. Formalize after dogfooding reveals what patterns are actually needed. Don't over-structure prematurely.

---

### 30. **Goal #12: Configurable Autonomy Levels**
**Decision:** Two levels - Interactive (L1) and Auto (L3)

**Levels:**
- **Level 1 (Interactive)** - Default
  - Claude creates plan, waits for user approval
  - Implements autonomously after approval
  - Good for critical work, learning, high-stakes projects

- **Level 3 (Auto)** - Fast mode
  - Claude creates plan and implements immediately
  - Only asks approval before final commit
  - Good for prototyping, personal projects, trusted changes

**Configuration:** `.agents/config/autonomy.yaml`
```yaml
mode: interactive  # or: auto
```

**Rationale:** Two levels cover 90% of use cases. Simpler than 5-level system. Can add L0, L2, L4 later if users request.

---

### 31. **Goal #13: Standard Folder Structures**
**Decision:** Defer to Priority 4

**Approach:**
- This project (py-ai-starter-kit) IS the template
- piv-swarm-example/ exists as reference
- After dogfooding, clean up and document as official template
- Build `piv init` command when patterns are proven

**Rationale:** Need 3-4 successful projects using PIV before formalizing template. Learn what works first, templatize later.

---

## Priority 3: Task #9 Complete ✓

**Decisions for Goals #10, #11, #12, #13**

---

### 32. **Goals #14-15: Smart Context Management & Cost Tracking**
**Decision:** Implement both in Priority 4 (Q4 2026)

**Goal #14: Smart Context Management**
- Layered loading (minimal → expand on-demand)
- Context caching (Anthropic prompt caching API)
- Progressive detail strategies
- Research complete in CONTEXT-OPTIMIZATION-RESEARCH.md

**Goal #15: Cost Tracking & Optimization**
- Track API costs per feature, session, project
- Budget alerts and spending limits
- Token usage dashboards
- Optimization suggestions based on data

**Rationale:** Research is complete. After Priority 1-3 are solid and patterns proven, implement advanced optimization. Data-driven cost management enables scale.

---

### 33. **Goal #16: Linear Integration**
**Decision:** Build in Priority 4 - Good for company rollout

**Features:**
- Auto-sync feature status to Linear tickets
- Link commits/PRs to tickets automatically
- Update ticket status from session state
- Management visibility into AI-assisted development

**Rationale:** Company adoption requires integration with existing tools. Linear is already used. Automation reduces friction for developers.

---

### 34. **Goals #17-18-19: Learning Systems (Deferred)**
**Decision:** Defer Pattern Library, Analytics, Onboarding to Priority 5+

**Deferred Goals:**
- Goal #17: Cross-Project Pattern Library
- Goal #18: Feedback Loop Analytics
- Goal #19: Developer Onboarding System

**Rationale:** Focus on core workflow (Priority 1-3) first. These are polish/adoption features. Build after PIV methodology is proven and stable. Reevaluate based on actual team needs in 2027.

---

### 35. **Goals #20-21: Provider Abstraction & Advanced Context (Deferred)**
**Decision:** Defer both to 2027+ reevaluation

**Deferred Goals:**
- Goal #20: Multi-Provider Abstraction (Anthropic vs AWS Bedrock)
- Goal #21: Advanced Context Optimization (overlaps with Goal #14)

**Rationale:**
- Anthropic API is sufficient (no need for provider abstraction)
- Goal #21 covered by Goal #14 (same research, same implementation)
- Very advanced features only needed at massive scale or if Anthropic has issues
- Wait and see if actually needed

---

## Priority 4: Task #10 Complete ✓

**Decisions for Goals #14, #15, #16, #17-18-19, #20-21**

---

## Final Goal Summary

### Priority 1 (Q1 2026) - 5 Goals
1. ✅ Prime Optimization
2. ✅ Spec Creation Process
3. ✅ Session Management
4. ✅ Multi-Session Architecture
5. ✅ Comprehensive Skills Tree

### Priority 2 (Q2 2026) - 3 Goals
6. ⚠️ JSON Features Tracking (Deferred - YAGNI)
7. ✅ TDD Culture (Training + Enforcement + Metrics)
9. ✅ Profiling & Optimization System

### Priority 3 (Q3 2026) - 4 Goals
10. ⚠️ Language Standards (YAGNI - Add when needed)
11. ⚠️ Ownership & Documentation (Defer formalization)
12. ✅ Configurable Autonomy (Two levels)
13. ⚠️ Standard Folder Structures (Defer to P4)

### Priority 4 (Q4 2026) - 3 Goals
14. ✅ Smart Context Management
15. ✅ Cost Tracking & Optimization
16. ✅ Linear Integration

### Deferred (2027+) - 6 Goals
17. ⚠️ Cross-Project Pattern Library
18. ⚠️ Feedback Loop Analytics
19. ⚠️ Developer Onboarding System
20. ⚠️ Multi-Provider Abstraction
21. ⚠️ Advanced Context Optimization (covered by #14)

**Total: 21 goals across 4 priorities + 6 deferred**

---

## Discussion Summary

### Total Decisions Made: 35

**Coverage:**
- ✅ Architecture & Strategy (5 decisions)
- ✅ State & Workflow (8 decisions)
- ✅ Token Management (5 decisions)
- ✅ Quality & Testing (5 decisions)
- ✅ Priority 1 Implementation (5 decisions)
- ✅ Priority 2-4 Goals (7 decisions)

**All 21 goals have clear direction ✓**

---

## Token Usage Check

**Current:** ~115K / 200K (57.5%)
**Status:** ✅ Well within budget
**Next:** Create comprehensive specification

**Architecture (3):**
- Primary goal, GSD relationship, swarm-lite approach

**State & Workflow (5):**
- State management, TDD enforcement, session breaks, resume strategy, commit strategy

**Token Management (4):**
- Prime modes, scope, deep usage, budget thresholds, caching

**Core Workflow (5):**
- Execute scope, validation timing, full test suite timing, coverage enforcement, TDD opt-out

**Quality (5):**
- TDD workflow, per-task validation, integration/e2e timing, coverage hybrid, opt-out validation

**Implementation (2):**
- Priority 1 order, Priority 2-4 deferred

---

## Next Steps

1. **Complete Task #6** - ✓ Deferred (goals documented in GOALS.md)
2. **Task #7** - Generate comprehensive project specification using /spec

---

## Notes

- Dogfooding approach validated: Use PIV to build PIV
- Manual multi-session during Priority 1 is acceptable and beneficial
- State structure is foundation - build it right from the start
- Full automation deferred to Priority 2-3 (pragmatic approach)

---

*Discussion in progress - last updated 2026-01-28*
